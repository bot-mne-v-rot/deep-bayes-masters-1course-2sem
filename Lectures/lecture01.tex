\subsection{Prerequisites}

\begin{edefn}
    Let $p(x \vert \theta)$ be distribution. Considering $\frac{\partial \log p(\hat{x} | \theta)}{\partial \theta}$, where $\hat{x} \sim p(x | \theta)$ and $\theta \in \RR^n$.

    \begin{equation*}
        \int p(\hat{x} | \theta) \frac{\partial \log p(\hat{x} | \theta)}{\partial \theta} d\hat{x} = \frac{\partial}{\partial \theta} \int p(\hat{x} | \theta) d\hat{x} = \frac{\partial}{\partial \theta} 1 = 0
    \end{equation*}

    And \textit{Fisher information matrix} is defined as
    \begin{equation*}
        \begin{aligned}
            F(\theta) &= \int p(\hat{x} | \theta) \left( \frac{\partial \log p(\hat{x} | \theta)}{\partial \theta} \right) \left( \frac{\partial \log p(\hat{x} | \theta)}{\partial \theta} \right)^T \dd \hat{x} \\
                      &= -\int p(\hat{x} | \theta) \left( \frac{\partial^2 \log p(\hat{x} | \theta)}{\partial \theta^2} \right) \dd \hat{x} \\
                      &= \mathbb{E}_{\hat{x} \sim p(x | \theta)} \left[ \left( \frac{\partial \log p(\hat{x} | \theta)}{\partial \theta} \right)^2 \right].
        \end{aligned}
    \end{equation*}
\end{edefn}

To prove the 2nd ``='' in the above definition, consider i,j-th element of the matrix:
\begin{equation*}
    \begin{aligned}
        &-\int p(\hat{x} | \theta) \left( \pdv{\log p(\hat{x} | \theta)}{\theta_j}{\theta_i} \right) \dd \hat{x} \\
        &=-\int p(\hat{x} | \theta) \left( \pdv{\theta_j}(\frac{1}{p(\hat{x} | \theta)}\pdv{p(\hat{x} | \theta)}{\theta_i}) \right) \dd \hat{x} \\
        &=\int p(\hat{x} | \theta) \left(\frac{1}{p(\hat{x} | \theta)}\right)^2 \left(\pdv{p(\hat{x} | \theta)}{\theta_j}\right)\left(\pdv{p(\hat{x} | \theta)}{\theta_i}\right) \dd \hat{x}
          - \int \pdv{p(\hat{x} | \theta)}{\theta_i}{\theta_j} \dd \hat{x} \\
        &= \int p(\hat{x} | \theta) \left( \pdv{\log p(\hat{x} | \theta)}{\theta_i} \right) \left( \pdv{\log p(\hat{x} | \theta)}{\theta_j} \right) \dd \hat{x} \\
        &\equiv F_{i,j}(\theta)
    \end{aligned}
\end{equation*}

\begin{eremark}
    $F(\theta) \succeq 0$, i.e. Fisher information matrix is positive semi-definite for all values of $\theta$.
    To prove this, denote by $g$ the gradient $\pdv{\log p(x\mid \theta)}{\theta}$ and note that for a vector $v$ we can write $v^T F(\theta) v = \int p(x\mid \theta) v^T gg^T v\dd x = \int p(x\mid \theta) (v^T g)^2 \dd x \ge 0$.
\end{eremark} \ 

Let $f(x)$ be a function, then its gradient may be expressed in the following peculiar way.
\[ 
    \grad f(x) \sim \argmax_{\delta} f(x + \delta)
    \text{ such that } \rho(x, x+ \delta) < \varepsilon \text{ and } \varepsilon>0 \text{ is sufficiently small }
\]
% % I tried, but this definition is too sloppy to be formalized.
% % Moreover, the original paper presents it the same way: https://arxiv.org/abs/1206.7051
% Formally,
% $$\grad f(x) = C(x) \lim_{\varepsilon \to 0+} [ \argmax_{\delta \in B_{\varepsilon}} f(x + \delta)]$$
% where $B_{\varepsilon} \subset \RR^n$ is the zero-centered ball with radius $\varepsilon$ and $C(x)$ is some positive-real-valued function.
% This is true because $f(x + \delta) = f(x) + \langle \grad f(x), \delta \rangle  + O(\norm{\delta}^2)$.
% $$\int p(x | \theta) g(x) dx = \MI(p(x | \theta)) = \MI(\theta)$$

\begin{edefn}
    % proposed in paper https://doi.org/10.1162/089976698300017746
    \textit{Natural gradient} is defined as
    \begin{equation*}
        \begin{aligned}
            \operatorname{natgrad} \ML \sim \argmax_{\delta} &\left( \ML(p (x \vert \theta + \delta) ) - \ML(p (x \vert \theta) ) \right) \\ 
            &\text{such that } \operatorname{KL} \left( p (x \vert \theta + \delta) \parallel p (x \vert \theta) \right) < \varepsilon
        \end{aligned}
    \end{equation*}
    Which is kinda reminds the upper <<definition>> of the gradient. And also we can easily compute it using the Fisher information matrix:
    \[ 
        \operatorname{natgrad} \ML(p(x \vert \theta)) = F^{-1}(\theta) \grad_\theta \ML(p(x \vert \theta))
    \]
\end{edefn}

\begin{eremark}
    Natural gradient is parametrisation-invariant.
\end{eremark}

\subsection{SVI}

Let's consider $$p(X, Z, \theta) = \prod_{i = 1}^n p(x_i, z_i | \theta) p(\theta),$$ where $x_i$ are observed, $z_i$ are hidden and $\theta$ are parameters. Our goal is to obtain $p(Z, \theta | X)$. Let us add some assumptions, such as $Z$ -- conjugate to $\theta$ and vice versa, and $p(x \vert z, \theta)$, $p(z \vert \theta)$ and $p(\theta)$ lie in exponential family. Then we can approximate $p(Z, \theta | X) \approx q(Z) q(\theta)$ by mean-field and from conjugacy and exponentiality conclude that
%integrable?
\begin{equation*}
    \begin{aligned}
        p(x, z | \theta) &= f(\theta) \rho(x, z) \exp (\theta^T h(x, z)) \\
        p(\theta) &= \frac{f(\theta)}{g(\nu_0, \eta_0)} \exp (\eta_0^T \theta)
    \end{aligned}
\end{equation*}

Therefore,
\begin{equation*}
    \begin{aligned}
        \log  q(Z) &= \EE_{q(\theta)} \log p(X, Z, \theta) + \const = \sum_{i = 1}^n \EE_{\theta} \log p(x_i, z_i | \theta) + \EE_\theta\log p(\theta) + \const \\
        &= \sum_{i = 1}^n \left( \EE_\theta \log f(\theta) + \log p(x_i, z_i) + \EE \theta^T h(x_i, z_i)\right) + \const \\
        &= \underbrace{\sum_{i = 1}^n \left(\log p(x_i, z_i) + \EE \theta^T h(x_i, z_i) \right)}_{eq. 1} + \const \\
        &= \sum_{i = 1}^n \log q(z_i) + \const,
    \end{aligned}
\end{equation*}

hence, $q(Z) = \prod_{i = 1}^n q(z_i)$. And
\begin{equation*}
    \begin{aligned}
           \log q(\theta) &= \EE_{q(Z)} \log p(X, Z, \theta) + \const = \sum_{i = 1}^n \EE_{z_i} \log p(x_i, z_i | \theta) + \EE_{Z} \log p(\theta) + \const = \\
        &= \underbrace{(n + \nu_0) \log f(\theta) + \theta^T \left( \eta_0 + \sum_{i = 1}^n \EE h(x_i, z_i) \right)}_{eq. 2} + \const.
    \end{aligned}
\end{equation*}

Note that $q(\theta)$ also lies in exponential family with understandable parameters. And so on each step of mean-field approximation we need to consider all the observed data, which is not very efficient for large datasets. \\

Let's take a look at
\begin{equation*}
    \log p(X) = \int q(Z) q(\theta) \log \frac{p(X, Z, \theta)}{q(Z) q(\theta)} dZ d\theta + \operatorname{KL}(q(Z) q(\theta) \parallel p(Z, \theta | X)) = \ML(q(Z), q(\theta)),
\end{equation*}

which is greater than the first term as the second term is non-negative. Now, instead of block-coordinate optimizations, we can use gradient optimizations.

\begin{equation*}
    \begin{aligned}
        q(\theta) &= f(\theta) \exp \left( \theta^T \left( \underbrace{\eta_0 + \sum_{i = 1}^n \EE_{z_i} h(x_1, z_i)}_{\eta_1}\right) \right) \frac{1}{g(\nu_0 + n, \eta_0 + \sum_{i = 1}^n \EE h(x_1, z_i))} \\
        &= \frac{f(\theta)}{f(\nu_1, \eta_1)} \exp \left( \theta^T \eta_1 \right),
    \end{aligned}
\end{equation*}

where $\nu_1 = \nu_0 + n$. So that
\begin{equation*}
    \begin{aligned}
        \ML(\eta_1) &= \int q(Z) q(\theta | \eta_1) \left[\log p(X, Z, \theta) - \log q(\theta, \eta_1) \right] dZ d\theta + \const \\
        &= \int q(Z) q(\theta | \eta_1) \left[ \sum_{i = 1}^n \log p(x_i, z_i | \theta) + \log p(\theta) - \log q(\theta | \eta_1) \right] dZ d\theta + \const \\
        &= \int q(Z) q(\theta | \eta_1) \big[ n \log f(\theta) + \sum_{i = 1}^n \log p(x_i, z_i) + \theta^T \big( \sum_{i = 1}^n h(x_i, z_i) + \nu_0 \log f(\theta) + \eta_0^T \theta -\\
        &\equad - \log g(\nu_0, \eta_0)\big) - (n + \nu_0) \log f(\theta) - \theta^T \eta_1 + \log g(\nu_1, \eta_1)\big] dZ d\theta + \const \\
        &= \int q(\theta | \eta_1) \left[ \theta^T \left( \sum_{i = 1}^n \EE h(x_i, z_i) + \eta_0 - \eta_1\right) + \log g(\nu_1, \eta_1)\right] d \theta \\
        &= \int q(\theta | \eta_1) \left[ \theta^T \left( \nu_0 - \eta_1 + \sum_{i = 1}^n \EE h(x_i, z_i)\right)\right] d \theta + \log g(\nu_1, \eta_1) + \const.
    \end{aligned}
\end{equation*}

so that
\begin{equation*}
    \frac{\partial}{\partial \eta_1} \ML(\eta_1) = \underbrace{\frac{\partial^2 \log g(\eta_1, \nu_1)}{\partial^2 \eta_1}}_{F(\eta_1)} \left(\eta_0 - \eta_1 + \sum_{i = 1}^n \EE h(x_i, z_i) \right),
\end{equation*}

hence, we can easily obtain the expression for the natural gradient and due to the unbiasedness introduce stochastic natural gradient:

\begin{equation*}
    \begin{aligned}
        \operatorname{natgrad} \MI(\eta_1) &= \eta_0 - \eta_1 + \sum_{i = 1}^n \EE h(x_i, z_i); \\
        \operatorname{stochnatgrad} \MI(\eta_1) &= \eta_0 - \eta_1 + n \EE_{z_j} h(x_j, z_j),
    \end{aligned}
\end{equation*} \ 

where $j \sim U\{1, \ldots, n\}$. And the iterative algorithm is

\begin{algorithm}
    \caption{SVI Algorithm}
    \begin{algorithmic}
        \State \textbf{Set} $j \sim U\{1, \ldots, n\}$;
        \State \textbf{Update} $\log q(z_j) = \log p(x_j, z_j) + \EE \theta^T h(x_j, z_j) + \const$;
        \State \textbf{Update} $\eta_1^{t+1} = \eta_1^t + \alpha \left( \eta_0 - \eta_1^t + \EE h(x_j, z_j) \right)$.
    \end{algorithmic}
\end{algorithm}

% \subsection{Seminar}

% And now once again about Fisher information. Let $p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)$, then we can say that $\MI_x(\mu)$ represents the information contained in $x$ about $\mu$. $\MI_x(\mu) \propto \frac{1}{\sigma^2}$.

% \begin{edefn}
%     The \textit{Fisher information} is defined as $$\MI(\theta) = \EE \left[ \left( \frac{\partial \log p(x | \theta)}{\partial \theta} \right)^2 \right] = -\EE \left[ \frac{\partial^2 \log p(x | \theta)}{\partial \theta^2} \right].$$
% \end{edefn}

% So for Gaussian distribution we have $\theta = \mu$,
% \begin{equation*}
%     \begin{aligned}
%         \log p(x | \mu) &= \log \frac{1}{\sqrt{2 \pi \sigma^2}}- \frac{(x - \mu)^2}{2\sigma^2}, \\
%         \frac{\partial \log p(x | \mu)}{\partial \mu} &= \frac{x - \mu}{\sigma^2}, \\
%         \MI(\theta) &= \frac{1}{\sigma^2}.
%     \end{aligned}
% \end{equation*}

% Alternative definition of Fisher information is
% \begin{enumerate}
%     \item $\MI_x(\theta) = - \EE(l''(\theta | x))$;
%     \item $\MI_x(\theta) = \Var (l'(\theta |x))$,
% \end{enumerate}

% where $l$ is a score function, $l(\theta | x) = \log p(x | \theta)$.
